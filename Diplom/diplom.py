# -*- coding: utf-8 -*-
"""DIPLOM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_dIx4NCN2FmQfP1yhJk8VDjvPjp_hW2-
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Импорт библиотек"""

#!pip install catboost
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
import tensorflow.keras.layers as L
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import re

"""# Фиксация эксперимента"""

RANDOM_SEED=42

"""# Просмотр данных"""

data = pd.read_csv("/content/drive/MyDrive/data-diplom.csv")
data

"""Сразу видно много пропусков, поэтому придется постараться убрать их по максимуму, и чтобы большая часть данных осталась"""

def mape(y_true, y_pred):
    return np.mean(np.abs((y_pred-y_true)/y_true))

"""# Работа с пропусками"""

data.isnull().sum()
fig, ax = plt.subplots(figsize=(10, 8))
sns_heatmap = sns.heatmap(data.isnull(),
                          yticklabels=False,
                          cbar=False,
                          cmap='viridis')

data = data.drop(["city","private pool","street",'mls-id','homeFacts','MlsId',"stories","schools","propertyType","fireplace"],axis=1)
data["PrivatePool"] = data["PrivatePool"].fillna("no")

"""В начале работы пришлось удалить много столбцов с данными, так как они не несли,по-моему мнению большой значимости. 
"private pool" - дублировался.
"mls-id" - имел большое количество пропусков.
"homefacts" - по моему мнению не нёс большой значимости.
"MlsId" - имел большое количество пропусков.
"PrivatePool" - заменил все пропуски на NO.


"""

data.isnull().sum()
fig, ax = plt.subplots(figsize=(10, 8))
sns_heatmap = sns.heatmap(data.isnull(),
                          yticklabels=False,
                          cbar=False,
                          cmap='viridis')

"""#### Разбор столбца Status"""

data["status"] = data["status"].str.replace("forsale","sale")
data["status"] = data["status"].str.replace("For sale","sale")
data["status"] = data["status"].str.replace("New construction","sale")
data["status"] = data["status"].str.replace("foreclosure","Active")
data["status"] = data["status"].str.replace("Lease/Purchase","sale")
data["status"] = data["status"].str.replace("Pre-Active / auction","sale")
data["status"] = data["status"].str.replace("Pending","sale")
data["status"] = data["status"].str.replace("Pre-Active","Active")
data["status"] = data["status"].str.replace("P","Active")
data["status"] = data["status"].str.replace("Under Contract Show","sale")
data["status"] = data["status"].str.replace(" / auction","sale")
data["status"] = data["status"].str.replace(" ","")
data["status"] = data["status"].str.replace("forsale","sale")
data = data[(data["status"] == "sale") | (data["status"] == "Active")].reset_index()
data.drop("index", axis = 1)
data["status"] = data["status"].str.replace("sale","0")
data["status"] = data["status"].str.replace("Active","1")
data["status"] = pd.to_numeric(data["status"], errors='coerce')
data["status"].isnull().sum()

"""#### Разбор столбца Baths"""

data["baths"] = data["baths"].str.replace("Baths","")
data["baths"] = data["baths"].str.replace(" ","")
data["baths"] = pd.to_numeric(data["baths"], errors='coerce')
data["baths"] = data["baths"].fillna(0)
data["baths"] = data["baths"].dropna()
data["baths"].isnull().sum()

"""#### Разбор столбца sqft"""

data["sqft"] = data["sqft"].str.replace("sqft","")
data["sqft"] = data["sqft"].str.replace(",","")
data["sqft"] = data["sqft"].str.replace("Total interior livable area:","")
data["sqft"] = data["sqft"].str.replace(".","")
data["sqft"] = data["sqft"].str.replace("-- ","")
data["sqft"] = data["sqft"].str.replace(" ","")
data["sqft"] = pd.to_numeric(data["sqft"], errors='coerce')
data["sqft"] = data["sqft"].fillna(0)
data["sqft"] = data["sqft"].dropna()
data["sqft"].isnull().sum()

"""### Разбор столбца beds"""

data["beds"] = data["beds"].str.replace("Beds","")
data["beds"] = data["beds"].str.replace(" ","")
data["beds"] = pd.to_numeric(data["beds"], errors='coerce')
data["beds"] = data["beds"].fillna(0)
data["beds"] = data["beds"].dropna()
data["beds"].isnull().sum()

"""## Разбор столбца zipcode

В данном столбце пропусков нет,тольковсе числа в формате "str", поэтому здесь нужно просто перевести всё в другой формат для лальнейшей обработки.
"""

data["zipcode"] = data["zipcode"].str.replace("-","")
data["zipcode"] = data["zipcode"].str.replace(" ","")
data["zipcode"] = pd.to_numeric(data["zipcode"], errors='coerce')
data["zipcode"] = data["zipcode"].fillna(0)
data["zipcode"].isnull().sum()

"""### Разбор столбца target"""

data["target"] = data["target"].str.replace("$","")
data["target"] = data["target"].str.replace(" ","")
data["target"] = data["target"].str.replace(",","")
data["target"] = data["target"].str.replace(".","")
data["target"] = data["target"].str.replace("+","")
data["target"] = data["target"].str.replace("/mo","")
data["target"] = data["target"].str.replace("1215-1437","1326")
data["target"] = data["target"].fillna(10000)
data["target"] = [float(x) for x in data["target"]]
data["target"].isnull().sum()

data["beds"].value_counts()

"""### Разбор столбца Privatepool"""

data["PrivatePool"] = data["PrivatePool"].str.replace("no","0")
data["PrivatePool"] = data["PrivatePool"].str.replace("yes","1")
data["PrivatePool"] = data["PrivatePool"].str.replace("Yes","1")
data["PrivatePool"] = pd.to_numeric(data["PrivatePool"], errors='coerce')

data.isnull().sum()
fig, ax = plt.subplots(figsize=(10, 8))
sns_heatmap = sns.heatmap(data.isnull(),
                          yticklabels=False,
                          cbar=False,
                          cmap='viridis')

data.isnull().sum()

data = pd.concat([data, pd.get_dummies(data["state"])], axis=1)
data = data.drop(["state"], axis=1)

data.info()

"""### Разбиение на тестовую и обучающую выборки"""

X = data.drop(["target"], axis=1)
y = data["target"]
#X = np.log(data.drop(["target"], axis=1))
#y = np.log(data["target"])

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, sep= "\n")

"""Проведем быстрый анализ данных для того, чтобы понимать, сможет ли с этими данными работать наш алгоритм."""

#посмотрим, как выглядят распределения числовых признаков
fig, ax = plt.subplots(3, 2, figsize=(10, 10))
sns.countplot(x = "status", data = data, ax = ax[0,0])
sns.countplot(x = "beds", data = data, ax = ax[0,1])
sns.countplot(x = "baths", data = data, ax = ax[1,0])
sns.countplot(x = "PrivatePool", data = data, ax = ax[1,1])
plt.plot(data["target"])

"""# Model 1: CatBoostRegressor"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=RANDOM_SEED)

from catboost import CatBoostRegressor
model = CatBoostRegressor(iterations = 5000,
                          random_seed = RANDOM_SEED,
                          eval_metric='MAPE',
                          custom_metric=['RMSE', 'MAE'],
                          od_wait=500,
                         )
model.fit(X_train, y_train,
         eval_set=(X_test, y_test),
         verbose_eval=100,
         use_best_model=True,
         plot=True
         )

test_predict_catboost = model.predict(X_test)
print(f"TEST mape: {(mape(y_test, test_predict_catboost)):0.2f}%")

test_predict_catboost

"""# Simple Dense NN"""

model_NN = Sequential()
model_NN.add(L.Dense(512, input_dim=data.shape[0], activation="relu"))
model_NN.add(L.Dropout(0.5))
model_NN.add(L.Dense(256, activation="relu"))
model_NN.add(L.Dropout(0.5))
model_NN.add(L.Dense(1, activation="linear"))

model_NN.summary()

# Compile model
optimizer = tf.keras.optimizers.Adam(0.01)
model_NN.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])

checkpoint = ModelCheckpoint('/content/drive/model_NN.hdf5' , monitor=['val_MAPE'], verbose=0  , mode='min')
earlystop = EarlyStopping(monitor='val_MAPE', patience=50, restore_best_weights=True,)
callbacks_list = [checkpoint, earlystop]

history = model_NN.fit(X_train, y_train,
                    batch_size=512,
                    epochs=500, # фактически мы обучаем пока EarlyStopping не остановит обучение
                    validation_data=(X_test, y_test),
                    callbacks=callbacks_list,
                    verbose=0,
                   )

plt.title('Loss')
plt.plot(history.history['MAPE'], label='train')
plt.plot(history.history['val_MAPE'], label='test')
plt.show();

test_predict_nn1 = model_NN.predict(X_test)
print(f"TEST mape: {(mape(y_test, test_predict_nn1[:,0]))*100:0.2f}%")

def mae(y_true, y_pred):
    return np.mean(abs(y_true - y_pred))

baseline_guess = np.median(y)

print('The baseline guess is a score of %0.2f' % baseline_guess)
print("Baseline Performance on the test set: MAE = %0.2f" % mae(y_test, baseline_guess))